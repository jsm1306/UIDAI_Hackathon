import json
with open('s:/UIDAI_Hackathon/spikeprediction.ipynb', 'r') as f:
    nb = json.load(f)

# Add dynamic weight adjustment cell
nb['cells'].append({
    'cell_type': 'code',
    'execution_count': None,
    'metadata': {},
    'outputs': [],
    'source': [
        '# Dynamic Weight Adjustment Based on Spike Probability\n',
        '# For test samples where P(spike) is high, upweight volatile XGBoost\n',
        '\n',
        '# Get spike probabilities for test set\n',
        'test_spike_proba = spike_classifier.predict_proba(X_test)[:, 1]\n',
        '\n',
        '# Define threshold for high spike probability\n',
        'spike_threshold = 0.7\n',
        'high_spike_mask = test_spike_proba > spike_threshold\n',
        '\n',
        'print(f\"High spike probability samples: {high_spike_mask.sum()} out of {len(test_spike_proba)} ({high_spike_mask.mean():.1%})\")\n',
        '\n',
        '# Example: Adjust weights for ensemble\n',
        '# Assuming we have predictions from different models\n',
        '# For high spike probability, increase weight of volatile models\n',
        '\n',
        '# Placeholder for model predictions (would come from main ensemble)\n',
        '# pred_stable = stable_ensemble.predict(X_test)\n',
        '# pred_volatile = volatile_ensemble.predict(X_test)\n',
        '\n',
        '# Dynamic weighting logic\n',
        'def dynamic_ensemble(pred_stable, pred_volatile, spike_proba, spike_thresh=0.7):\n',
        '    \"\"\"Combine predictions with dynamic weighting based on spike probability\"\"\"\n',
        '    # Base weights\n',
        '    w_stable = 0.6\n',
        '    w_volatile = 0.4\n',
        '    \n',
        '    # Adjust weights for high spike probability\n',
        '    high_spike = spike_proba > spike_thresh\n',
        '    w_stable_adj = np.where(high_spike, w_stable * 0.5, w_stable)  # Reduce stable weight\n',
        '    w_volatile_adj = np.where(high_spike, w_volatile * 1.5, w_volatile)  # Increase volatile weight\n',
        '    \n',
        '    # Normalize weights\n',
        '    total_weight = w_stable_adj + w_volatile_adj\n',
        '    w_stable_adj /= total_weight\n',
        '    w_volatile_adj /= total_weight\n',
        '    \n',
        '    return w_stable_adj * pred_stable + w_volatile_adj * pred_volatile\n',
        '\n',
        'print(\"Dynamic weighting function defined.\")\n',
        'print(f\"For high spike prob (> {spike_threshold}): Stable weight reduced by 50%, Volatile weight increased by 50%\")'
    ]
})

# Add spike-specific model cell
nb['cells'].append({
    'cell_type': 'code',
    'execution_count': None,
    'metadata': {},
    'outputs': [],
    'source': [
        '# Train Spike-Specific Model\n',
        '# Model trained only on known spike dates (or top N% high enrollment days)\n',
        '\n',
        '# Identify spike training data\n',
        'spike_train_mask = (df_agg[\"date\"].dt.month <= 10) & (df_agg[\"spike_label\"] == 1)\n',
        'spike_X_train = X[spike_train_mask]\n',
        'spike_y_train = df_agg.loc[spike_train_mask, \"total_enrollment\"]\n',
        '\n',
        'print(f\"Spike training samples: {len(spike_X_train)}\")\n',
        '\n',
        '# Alternative: Train on top 10% enrollment days\n',
        '# enrollment_percentile = df_agg[df_agg[\"date\"].dt.month <= 10][\"total_enrollment\"].quantile(0.9)\n',
        '# high_enrollment_mask = (df_agg[\"date\"].dt.month <= 10) & (df_agg[\"total_enrollment\"] > enrollment_percentile)\n',
        '# high_X_train = X[high_enrollment_mask]\n',
        '# high_y_train = df_agg.loc[high_enrollment_mask, \"total_enrollment\"]\n',
        '\n',
        '# Train XGBoost on spike data\n',
        'spike_model = xgb.XGBRegressor(\n',
        '    n_estimators=200,\n',
        '    learning_rate=0.05,\n',
        '    max_depth=8,\n',
        '    random_state=42\n',
        ')\n',
        '\n',
        'if len(spike_X_train) > 0:\n',
        '    spike_model.fit(spike_X_train, spike_y_train)\n',
        '    print(\"Spike-specific model trained.\")\n',
        'else:\n',
        '    print(\"No spike training data available.\")\n',
        '\n',
        '# Function to use spike model when classifier predicts spike\n',
        'def spike_aware_prediction(X, spike_classifier, spike_model, base_model, spike_thresh=0.7):\n',
        '    \"\"\"Use spike model when spike probability is high, otherwise use base model\"\"\"\n',
        '    spike_proba = spike_classifier.predict_proba(X)[:, 1]\n',
        '    spike_mask = spike_proba > spike_thresh\n',
        '    \n',
        '    predictions = np.zeros(len(X))\n',
        '    \n',
        '    # Use spike model for high probability\n',
        '    if spike_mask.sum() > 0:\n',
        '        predictions[spike_mask] = spike_model.predict(X[spike_mask])\n',
        '    \n',
        '    # Use base model for low probability\n',
        '    if (~spike_mask).sum() > 0:\n',
        '        predictions[~spike_mask] = base_model.predict(X[~spike_mask])\n',
        '    \n',
        '    return predictions, spike_proba\n',
        '\n',
        'print(\"Spike-aware prediction function defined.\")\n',
        'print(f\"Will use spike model when P(spike) > {spike_thresh}\")'
    ]
})

with open('s:/UIDAI_Hackathon/spikeprediction.ipynb', 'w') as f:
    json.dump(nb, f, indent=1)